import json
import time
import asyncio
from typing import AsyncGenerator, Dict, Any, Optional
import structlog
from langchain_core.messages import HumanMessage

from app.core.graph.workflow import build_medical_graph

logger = structlog.get_logger(__name__)

class ChatService:
    """
    Chat ä¸šåŠ¡æœåŠ¡å±‚
    è´Ÿè´£åè°ƒ LangGraph æ‰§è¡Œã€äº‹ä»¶è§£æã€UX ä¼˜åŒ–é€»è¾‘ã€‚
    [V2.0 Refactor] æ”¯æŒåŒè½¨å¹¶è¡Œæµå¼ (Fast Track + Expert Track)
    """
    def __init__(self, graph=None):
        # æ”¯æŒä¾èµ–æ³¨å…¥ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€ Graph
        self.graph = graph if graph else build_medical_graph()

    async def stream_events(self, message: str, session_id: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æ‰§è¡Œå¯¹è¯æµå¹¶ç”Ÿæˆä¸šåŠ¡äº‹ä»¶
        """
        inputs = {
            "symptoms": message,
            "user_input": message,
            "current_turn_input": message,
            "retrieval_query": message,
            "event": {
                "event_type": "SYMPTOM_DESCRIPTION",
                "payload": {"session_id": session_id},
                "raw_input": message,
                "timestamp": time.time(),
            },
            "messages": [HumanMessage(content=message)],
            "patient_id": "p_guest",
            "session_id": session_id or "unknown",
            "medical_history": "" 
        }
        
        # 1. Yield Initial Status
        yield {"type": "status", "content": "ç³»ç»Ÿå·²æ¥æ”¶è¯·æ±‚ï¼Œæ­£åœ¨å¯åŠ¨åŒè½¨åˆ†æ..."}
        
        node_start_times = {}
        fast_track_done = False
        
        try:
            # 2. Stream LangGraph Events
            config = {"configurable": {"thread_id": session_id}}
            async for event in self.graph.astream_events(inputs, config=config, version="v2"):

                kind = event["event"]
                node_name = event.get("metadata", {}).get("langgraph_node", "")
                run_id = event.get("run_id")
                
                # [Logic] Node Timing Start
                if kind == "on_chain_start" and node_name:
                    node_start_times[run_id] = time.time()
                    if self._is_key_node(node_name):
                        yield {"type": "thought", "content": f"â¡ï¸ è¿›å…¥èŠ‚ç‚¹: {node_name}"}

                # [Logic] Node Timing End
                if kind == "on_chain_end" and node_name:
                    if run_id in node_start_times:
                        duration = time.time() - node_start_times[run_id]
                        if self._is_key_node(node_name):
                            yield {"type": "thought", "content": f"âœ… èŠ‚ç‚¹å®Œæˆ: {node_name} (è€—æ—¶: {duration:.4f}s)"}

                # [UX] Status Updates
                if kind == "on_chain_start":
                    status_msg = self._get_status_message(node_name)
                    if status_msg:
                        yield {"type": "status", "content": status_msg}
                
                # [Logic] Fast Track Streaming (Real-time tokens)
                # ç›‘å¬ fast_track èŠ‚ç‚¹å†…éƒ¨çš„ LLM æµå¼è¾“å‡º
                if kind == "on_chat_model_stream" and node_name == "fast_track":
                    content = event["data"]["chunk"].content
                    if content:
                        yield {"type": "token", "content": content}
                        fast_track_done = True

                # [Logic] Fast Track Completion
                if kind == "on_chain_end" and node_name == "fast_track":
                    # Fallback: å¦‚æœ LLM ä¸æ”¯æŒæµå¼ï¼Œåˆ™ä¸€æ¬¡æ€§è¾“å‡ºç»“æœ
                    if not fast_track_done:
                        output = event["data"].get("output")
                        content = ""
                        if output and isinstance(output, dict):
                            content = output.get("fast_response") or output.get("content")
                        
                        if content:
                            yield {"type": "token", "content": content}
                    
                    # [UX] è§†è§‰å ä½ä¼˜åŒ–ï¼šFast Track ç»“æŸåï¼Œç«‹å³é€šçŸ¥å‰ç«¯å±•ç¤º "ä¸“å®¶ç ”åˆ¤ä¸­" çŠ¶æ€
                    # å¡«è¡¥ Fast Track ä¸ Expert Crew ä¹‹é—´çš„ 10s+ ç©ºç™½æœŸ
                    yield {"type": "status", "content": "expert_calculating"}

                # [Logic] Expert Crew Output (Final Result)
                # å½“ expert_crew å®Œæˆæ—¶ï¼Œè·å–å…¶è¾“å‡ºå¹¶å±•ç¤º
                if kind == "on_chain_end" and node_name == "expert_crew":
                    output = event["data"].get("output")
                    # CrewAI/LangGraph è¿™é‡Œçš„è¾“å‡ºç»“æ„é€šå¸¸æ˜¯ state çš„æ›´æ–°
                    # å¦‚æœæ˜¯ expert_crewï¼Œå®ƒè¿”å›çš„æ˜¯ {"messages": [AIMessage(...)]}
                    
                    final_msg = ""
                    if output and isinstance(output, dict):
                        if "messages" in output:
                            msgs = output["messages"]
                            if msgs and len(msgs) > 0:
                                final_msg = msgs[-1].content
                        # å…¼å®¹ç›´æ¥è¿”å› content çš„æƒ…å†µ
                        elif "content" in output:
                            final_msg = output["content"]

                    if final_msg:
                        # åœ¨ Fast Response åè¿½åŠ ä¸“å®¶åˆ†æ
                        separator = "\n\n---\n\n**ğŸ¥ ä¸‰ç”²ä¸“å®¶ç»„ä¼šè¯ŠæŠ¥å‘Š**:\n\n"
                        yield {"type": "token", "content": separator}
                        
                        # æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœè¾“å‡ºä¸“å®¶é•¿æ–‡ï¼Œé¿å…ç¬é—´åˆ·å±
                        chunk_size = 20
                        for i in range(0, len(final_msg), chunk_size):
                            chunk = final_msg[i:i+chunk_size]
                            yield {"type": "token", "content": chunk}
                            await asyncio.sleep(0.01)

                # [Logic] Guardrail Block
                if kind == "on_chain_end" and (node_name == "guard" or node_name == "safety_audit"):
                     output = event["data"].get("output")
                     if isinstance(output, dict) and output.get("status") == "blocked":
                         yield {"type": "error", "content": "è¯·æ±‚è¢«åŒ»ç–—å®‰å…¨æŠ¤æ æ‹¦æˆª"}

        except Exception as e:
            logger.error("chat_stream_error", error=str(e))
            yield {"type": "error", "content": f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}"}
            
        yield {"type": "done", "content": "COMPLETE"}

    def _is_key_node(self, node_name: str) -> bool:
        """Helper: å†³å®šæ˜¯å¦æ˜¾ç¤ºèŠ‚ç‚¹è€—æ—¶"""
        return node_name in ["guard", "triage_router", "fast_track", "expert_crew", "quality_gate", "persistence", "safety_audit"]

    def _get_status_message(self, node_name: str) -> Optional[str]:
        """Helper: è·å–èŠ‚ç‚¹çŠ¶æ€æ–‡æ¡ˆ"""
        mapping = {
            "guard": "æ­£åœ¨è¿›è¡Œå®‰å…¨åˆè§„æ£€æŸ¥...",
            "safety_audit": "æ­£åœ¨è¿›è¡ŒäºŒæ¬¡åŒ»ç–—ä¸ç”¨è¯å®‰å…¨å®¡æŸ¥...",
            "triage_router": "æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†è¯Š...",
            "fast_track": "âš¡ æ­£åœ¨è°ƒç”¨æœ¬åœ°çŸ¥è¯†åº“å¿«é€Ÿå“åº”...",
            "expert_crew": "ğŸ¥ ä¸“å®¶ç»„æ­£åœ¨è¿›è¡Œå¤šå­¦ç§‘ä¼šè¯Š(MDT)...",
            "quality_gate": "æ­£åœ¨è¿›è¡ŒåŒ»ç–—è´¨æ§...",
            "summarize_history": "æ­£åœ¨æ±‡æ€»å†å²ç—…å†..."
        }
        return mapping.get(node_name)
