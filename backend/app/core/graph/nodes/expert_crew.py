import os
import time
import asyncio
import structlog
from typing import Dict, Any, List
from app.core.graph.state import AgentState
from app.core.monitoring.tracing import monitor_node
from app.core.config import settings
from app.core.models.vram_manager import vram_manager, vram_auto_clear
from app.core.llm.llm_factory import SmartRotatingLLM, get_fast_llm
from app.agents.factory import get_department_factory # [NEW] Import DepartmentAgentFactory
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.runnables.config import RunnableConfig

logger = structlog.get_logger(__name__)

def _build_chat_history(state: AgentState) -> List[Any]:
    """
    æ„å»º AgentExecutor æ‰€éœ€çš„ chat_historyã€‚
    å…è®¸ç¼ºå¤±ï¼Œè‡³å°‘è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å… MessagesPlaceholder ç¼ºå‚æŠ¥é”™ã€‚
    """
    history = state.get("messages", [])
    if isinstance(history, list):
        return history
    return []

async def run_dynamic_specialist_node(department: str, state: AgentState, config: RunnableConfig = None) -> Dict[str, Any]:
    """è¿è¡ŒåŠ¨æ€ä¸“å®¶èŠ‚ç‚¹ (Using DepartmentAgentFactory)"""
    try:
        # [Refactor] Use DepartmentAgentFactory instead of MedicalExpertCrew
        factory = get_department_factory()
        
        # 1. Create Agent dynamically
        agent_executor = factory.create_agent(department)
        
        # 2. Prepare Input
        symptoms = state.get("symptoms", "")
        history = state.get("medical_history", "")
        audit_retry_count = state.get("audit_retry_count", 0)
        audit_feedback = state.get("audit_feedback")
        
        input_text = f"ã€æ‚£è€…ä¸»è¯‰ã€‘ï¼š{symptoms}\nã€å†å²ç—…å†/ä¸Šä¸‹æ–‡ã€‘ï¼š{history}"
        
        if audit_retry_count > 0 and audit_feedback:
             input_text += f"\n\nã€âš ï¸ å®¡è®¡é©³å›ä¿®æ­£æŒ‡ä»¤ã€‘\nä¸Šä¸€æ¬¡çš„è¯Šæ–­æœªé€šè¿‡åˆè§„å®¡è®¡ï¼ŒåŸå› å¦‚ä¸‹ï¼š\n{audit_feedback}\n\nè¯·åŠ¡å¿…é’ˆå¯¹ä¸Šè¿°é—®é¢˜è¿›è¡Œä¿®æ­£ã€‚"

        # 3. Invoke Agent
        # agent_executor.invoke returns a dict with "output" key
        result = await agent_executor.ainvoke({
            "input": input_text,
            "chat_history": _build_chat_history(state),
        })
        result_str = result.get("output", str(result))
        
        # [Legacy Support] Extract Persona/Evidence if needed (Currently simplified)
        # Assuming the new factory agent returns clean text. 
        # If we need structured output (Persona/Evidence), we should update the prompt in factory.py later.
        
        persona_updates = []
        evidence_updates = []

        return {
            f"specialist_{department}_output": result_str,
            "persona_update_proposals": persona_updates,
            "evidence_chain": evidence_updates
        }
    except Exception as e:
        logger.error(f"specialist_failed_{department}", error=str(e))
        return {f"specialist_{department}_output": f"Error: Specialist {department} failed to respond. ({str(e)})"}

async def run_pharmacist_node(state: AgentState, config: RunnableConfig = None) -> Dict[str, Any]:
    """è¿è¡Œè¯å‰‚å¸ˆèŠ‚ç‚¹"""
    # æš‚æ—¶ä¿æŒåŸæœ‰é€»è¾‘ï¼Œæˆ–è€…ä¹Ÿè¿ç§»åˆ° Factory (å¦‚æœæœ‰è¯å‰‚ç§‘)
    # æ—¢ç„¶ Factory é‡Œæœ‰ "è¯å‰‚ç§‘_ç²¾å‡†åŒ»å­¦", æˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨ Factory
    try:
        factory = get_department_factory()
        # å°è¯•æŸ¥æ‰¾è¯å‰‚ç§‘ï¼Œå¦‚æœæ²¡æœ‰åˆ™ fallback åˆ°é»˜è®¤
        dept = "è¯å‰‚ç§‘_ç²¾å‡†åŒ»å­¦"
        if not factory.get_department_config(dept):
            # Fallback logic if needed, but registry has it.
            pass
            
        agent_executor = factory.create_agent(dept)
        
        symptoms = state.get("symptoms") or state.get("user_input") or ""
        history = state.get("medical_history") or ""
        diag_output = state.get("diagnostician_output", "") # Ideally we should pass the diagnosis to pharmacist
        
        input_text = f"è¯·å®¡æ ¸ä»¥ä¸‹è¯Šæ–­å’Œç”¨è¯å»ºè®®çš„å®‰å…¨æ€§ï¼š\n\nã€æ‚£è€…æƒ…å†µã€‘{symptoms}\n{history}\n\nã€åˆæ­¥è¯Šæ–­ã€‘\n{diag_output}"
        
        result = await agent_executor.ainvoke({
            "input": input_text,
            "chat_history": _build_chat_history(state),
        })
        return {"pharmacist_output": result.get("output", str(result))}

    except Exception as e:
        logger.error("pharmacist_failed", error=str(e))
        # Fallback to old implementation if factory fails or just return error
        return {"pharmacist_output": f"Error: Pharmacist review failed. ({str(e)})"}


@monitor_node("parallel_expert_crew")
# @vram_auto_clear(force=True) # [Optimization] Disabled for Cloud-Only Mode
async def parallel_expert_crew_node(state: AgentState, config: RunnableConfig = None) -> Dict[str, Any]:
    """å¹¶è¡Œä¸“å®¶ç»„èŠ‚ç‚¹"""
    print(f"[DEBUG] Node ParallelExpertCrew Start")
    logger.info("node_start", node="parallel_expert_crew")
    start = time.time()
    
    try:
        # [Optimization] Pre-inference VRAM Check
        # Ensure we have enough memory for parallel experts (approx 1500MB headroom)
        # await vram_manager.orchestrate_pre_inference_async(required_mb=1500)
        
        departments = state.get("departments", [])
        if not departments:
             departments = ["General Practice"]
             
        logger.info("expert_dynamic_dispatch", departments=departments)
        
        # [Debug] Trace Context
        symptoms = state.get("symptoms") or "MISSING"
        history = state.get("medical_history") or "MISSING"
        user_input = state.get("user_input") or "MISSING"
        logger.info(f"ğŸ” [ExpertCrew-Debug] Context Check: symptoms='{symptoms}' history='{history}' user_input='{user_input}'")
        
        # [Optim] Enable parallelism for Cloud-Only Mode
        # We use asyncio.gather to run specialists concurrently since we are using Cloud LLM.
        tasks = []
        for dept in departments:
            tasks.append(run_dynamic_specialist_node(dept, state, config))
            
        specialist_results = await asyncio.gather(*tasks)

        # Aggregate first to pass to pharmacist
        diag_output_list = []
        all_evidence = []
        all_persona_updates = []

        for i, dept in enumerate(departments):
            res = specialist_results[i]
            # Extract text output
            output_key = f"specialist_{dept}_output"
            text_out = res.get(output_key, str(res))
            diag_output_list.append(f"### {dept} Specialist:\n{text_out}")
            
            # Aggregate Evidence & Persona Updates
            if "evidence_chain" in res:
                all_evidence.extend(res["evidence_chain"])
            if "persona_update_proposals" in res:
                all_persona_updates.extend(res["persona_update_proposals"])
            
        diag_output = "\n\n".join(diag_output_list)
        
        # Inject diagnosis into state for pharmacist (temp)
        state["diagnostician_output"] = diag_output
        
        pharm_result = await run_pharmacist_node(state, config)
        
        logger.info(f"parallel_expert_crew_finished", tasks_count=len(specialist_results))
        
        return {
            "diagnostician_output": diag_output,
            "evidence_chain": all_evidence,
            "persona_update_proposals": all_persona_updates,
            "content": diag_output, # [Compatibility] For ChatService
            **pharm_result
        }

    except Exception as e:
        error_msg = str(e)
        if "No available LLM" in error_msg or "Local fallback disabled" in error_msg or "403" in error_msg:
            logger.warning("expert_crew_fallback_triggered", error=error_msg)
            local_llm = get_fast_llm(temperature=0.3, prefer_local=True)
            
            symptoms = state.get("symptoms", "")
            history = state.get("medical_history", "æš‚æ— å†å²ç—…å†")
            
            fallback_prompt = f"""ã€è¯šå®é™çº§æ¨¡å¼ã€‘
å½“å‰äº‘ç«¯ä¸“å®¶è¯Šæ–­æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œç³»ç»Ÿå·²åˆ‡æ¢è‡³æœ¬åœ°åŸºç¡€æ¨¡å‹ä¸ºæ‚¨æä¾›é—­ç¯å›å¤ã€‚
æˆ‘ä»¬å°†ç»“åˆæ‚¨çš„ä¸»è¯‰å’Œç³»ç»Ÿæ£€ç´¢åˆ°çš„åŒ»ç–—çŸ¥è¯†åº“å†…å®¹ä¸ºæ‚¨æä¾›åŸºç¡€å‚è€ƒã€‚

ã€æ‚£è€…ä¸»è¯‰ã€‘ï¼š{symptoms}
ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡/ç—…å†ã€‘ï¼š{history}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºä¸€äº›åŸºç¡€çš„å¥åº·å»ºè®®ï¼ˆéä¸“ä¸šè¯Šæ–­ï¼‰ã€‚
è¦æ±‚ï¼š
1. æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·è¿™æ˜¯æœ¬åœ°æ¨¡å‹çš„åˆæ­¥å»ºè®®ï¼Œéä¸“å®¶ä¼šè¯Šç»“è®ºã€‚
2. å»ºè®®ç”¨æˆ·åœ¨æ¡ä»¶å…è®¸æ—¶é‡æ–°å’¨è¯¢æˆ–å‰å¾€åŒ»é™¢ã€‚
3. ä¿æŒä¸“ä¸šã€ä¸¥è°¨ä¸”æ¸©é¦¨ã€‚
"""
            try:
                response = await local_llm.ainvoke(fallback_prompt)
                return {
                    "diagnostician_output": f"[æœ¬åœ°æ¨¡å‹é—­ç¯å›å¤]\n{response.content}",
                    "pharmacist_output": "ä¸“å®¶ç»„æœåŠ¡é™çº§ï¼Œæš‚æ— è¯¦ç»†ç”¨è¯å®¡æŸ¥ã€‚",
                    "auditor_output": "ä¸“å®¶ç»„æœåŠ¡é™çº§ï¼Œæš‚æ— è¯¦ç»†åˆè§„å®¡è®¡ã€‚",
                    "status": "downgraded",
                    "is_downgraded": True
                }
            except Exception as local_err:
                logger.error("local_fallback_failed", error=str(local_err))
                raise e
        raise e
