"""
âš ï¸ DEPRECATED: This script is deprecated and may cause data inconsistency.
Please use 'rebuild_milvus_v4_aligned.py' for production ingestion to ensure 
Milvus IDs are aligned with PostgreSQL medical_chunks table.

Standard Ingestion Flow:
1. Load data into PostgreSQL 'medical_chunks' first.
2. Use 'rebuild_milvus_v4_aligned.py' to sync SQL data to Milvus.
"""
import json
import torch
import time
import os
import sys
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from torch.utils.data import DataLoader, Dataset
from app.core.config import settings

# ================= é…ç½®ä¿¡æ¯ (Configuration) =================
# æ¨¡å‹è·¯å¾„
MODEL_PATH = os.getenv("EMBEDDING_MODEL_PATH", os.path.join(settings.PROJECT_ROOT, "models", "Qwen3-Embedding-0.6B"))
# æ•°æ®è·¯å¾„
JSONL_FILE = os.path.join(settings.PROJECT_ROOT, "data", "augmented_only.jsonl")
# è¿›åº¦è®°å½•æ–‡ä»¶
CHECKPOINT_FILE = "ingest_checkpoint.txt"
# Milvus é…ç½®
MILVUS_HOST = settings.MILVUS_HOST
MILVUS_PORT = settings.MILVUS_PORT
COLLECTION_NAME = "huatuo_knowledge"  # åŒ»ç–—çŸ¥è¯†åº“é›†åˆåç§°
DIMENSION = 1024  # æ¨¡å‹ç»´åº¦

# æ€§èƒ½å‚æ•°
BATCH_SIZE = 16
MAX_LENGTH = 512
INSERT_BUFFER_SIZE = 1000
NUM_WORKERS = 0 # é¿å…å¤šè¿›ç¨‹é—®é¢˜ï¼Œå…ˆè®¾ä¸º0

# ==========================================================

class MedicalDataset(Dataset):
    """
    åŒ»ç–—æ•°æ®é›†ç±» (Medical Dataset Class)
    ç”¨äºåŠ è½½å’Œå¤„ç† augmented_only.jsonl æ–‡ä»¶ã€‚
    """
    def __init__(self, file_path, start_line=0):
        """
        åˆå§‹åŒ–æ•°æ®é›† (Initialize Dataset)
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            start_line: èµ·å§‹è¡Œå·ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ 
        """
        self.data = []
        print(f"æ­£åœ¨åŠ è½½æ•°æ®é›† (ä»ç¬¬ {start_line} è¡Œå¼€å§‹)...")
        if not os.path.exists(file_path):
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
            return
            
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i < start_line:
                    continue
                self.data.append(line)
        print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(self.data)} æ¡æ•°æ®å¾…å¤„ç†")

    def __len__(self):
        """
        è·å–æ•°æ®é›†å¤§å° (Get Dataset Length)
        """
        return len(self.data)

    def __getitem__(self, idx):
        """
        è·å–å•æ¡æ•°æ® (Get Item)
        æå– jsonl ä¸­çš„ text å­—æ®µä½œä¸ºå‘é‡åŒ–å†…å®¹ã€‚
        """
        line = self.data[idx]
        try:
            item = json.loads(line)
            # ä¼˜å…ˆä½¿ç”¨ text å­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç»„åˆé—®é¢˜å’Œç­”æ¡ˆ
            text = item.get("text", "")
            if not text:
                # å°è¯•å…¶ä»–å­—æ®µå¹¶ç»„åˆ
                q = item.get("é—®é¢˜", "")
                a = item.get("ç­”æ¡ˆ", "")
                text = f"é—®é¢˜ï¼š{q} ç­”æ¡ˆï¼š{a}"
            
            # åŒæ—¶è¿”å›å…ƒæ•°æ®ä»¥ä¾¿å­˜å…¥æ•°æ®åº“ (è¿™é‡Œç®€åŒ–ä»…è¿”å›æ–‡æœ¬ï¼Œå¦‚æœéœ€è¦å­˜ metadata éœ€è¦ä¿®æ”¹ Dataset è¿”å›ç»“æ„)
            return text
        except:
            return ""

def load_checkpoint():
    """
    åŠ è½½è¿›åº¦æ–­ç‚¹ (Load Checkpoint)
    
    Returns:
        int: ä¸Šæ¬¡å¤„ç†åˆ°çš„è¡Œå·
    """
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            try:
                return int(f.read().strip())
            except ValueError:
                return 0
    return 0

def save_checkpoint(line_idx):
    """
    ä¿å­˜è¿›åº¦æ–­ç‚¹ (Save Checkpoint)
    
    Args:
        line_idx: å½“å‰å¤„ç†åˆ°çš„è¡Œå·
    """
    with open(CHECKPOINT_FILE, 'w') as f:
        f.write(str(line_idx))

def connect_milvus():
    """
    è¿æ¥ Milvus å¹¶åˆå§‹åŒ–é›†åˆ (Connect Milvus & Init Collection)
    
    Returns:
        Collection: Milvus é›†åˆå¯¹è±¡
    """
    print(f"æ­£åœ¨è¿æ¥ Milvus ({MILVUS_HOST}:{MILVUS_PORT})...")
    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    
    if utility.has_collection(COLLECTION_NAME):
        print(f"é›†åˆ {COLLECTION_NAME} å·²å­˜åœ¨ï¼ŒåŠ è½½ä¸­...")
        collection = Collection(COLLECTION_NAME)
        collection.load()
        return collection
    
    print(f"åˆ›å»ºæ–°é›†åˆ {COLLECTION_NAME}...")
    fields = [
        # ä¸»é”® IDï¼Œè‡ªåŠ¨å¢é•¿
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        # æ–‡æœ¬å†…å®¹ï¼Œå­˜å‚¨åŸå§‹ QA å¯¹
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=5000), # ç¨å¾®è°ƒå°ä¸€ç‚¹é¿å…è¿‡å¤§
        # å‘é‡æ•°æ®
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)
    ]
    schema = CollectionSchema(fields, "Smart Hospital Medical Knowledge Base")
    collection = Collection(COLLECTION_NAME, schema)
    
    # åˆ›å»ºç´¢å¼•
    print("åˆ›å»ºå‘é‡ç´¢å¼•...")
    index_params = {
        "metric_type": "COSINE", 
        "index_type": "HNSW", 
        "params": {"M": 16, "efConstruction": 256}
    }
    collection.create_index(field_name="vector", index_params=index_params)
    collection.load()
    return collection

def main():
    """
    ä¸»å‡½æ•° (Main Function)
    æ‰§è¡Œæ•°æ®åŠ è½½ã€å‘é‡åŒ–å’Œå…¥åº“æµç¨‹ã€‚
    """
    # 1. è·å–ä¸Šæ¬¡è¿›åº¦
    start_line = load_checkpoint()
    print(f"ğŸ”„ æ£€æµ‹åˆ°æ–­ç‚¹ï¼šä»ç¬¬ {start_line} è¡Œç»§ç»­è¿è¡Œ")

    # 2. å‡†å¤‡æ¨¡å‹
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_PATH}...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModel.from_pretrained(
            MODEL_PATH, trust_remote_code=True, torch_dtype=torch.float16 if device=="cuda" else torch.float32
        ).to(device)
        model.eval()
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. è¿æ¥æ•°æ®åº“
    try:
        collection = connect_milvus()
    except Exception as e:
        print(f"Milvus è¿æ¥å¤±è´¥: {e}")
        return
    
    # 4. åŠ è½½æ•°æ®
    dataset = MedicalDataset(JSONL_FILE, start_line=start_line)
    if len(dataset) == 0:
        print("æ²¡æœ‰æ•°æ®éœ€è¦å¤„ç†ï¼Œé€€å‡ºã€‚")
        return
        
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=(device=="cuda"))

    texts_buffer = []
    vectors_buffer = []
    start_time = time.time()
    total_processed_this_run = 0
    
    pbar = tqdm(total=len(dataset), desc="ğŸš€ å‘é‡åŒ–è¿›åº¦", unit="æ¡")

    try:
        with torch.no_grad():
            for batch in dataloader:
                # è¿‡æ»¤ç©ºæ•°æ®
                batch = [t for t in batch if t and len(t) > 5] 
                if not batch: 
                    pbar.update(BATCH_SIZE) # å³ä½¿è·³è¿‡ä¹Ÿè¦æ›´æ–°è¿›åº¦æ¡
                    continue

                # Tokenize & Embedding
                inputs = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt").to(device)
                outputs = model(**inputs)
                
                # è·å–å‘é‡ (Last Token Poolingï¼Œä¸å‚è€ƒä»£ç ä¸€è‡´)
                embeddings = outputs.last_hidden_state[:, -1, :]
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                vectors = embeddings.to(torch.float32).cpu().numpy().tolist()
                
                texts_buffer.extend(batch)
                vectors_buffer.extend(vectors)

                # è¾¾åˆ° buffer å¤§å°ï¼Œå†™å…¥ Milvus å¹¶ä¿å­˜è¿›åº¦
                if len(texts_buffer) >= INSERT_BUFFER_SIZE:
                    # è¡¥å…… Metadata (department, disease, source) ä»¥åŒ¹é… Schema
                    department_buffer = ["General"] * len(texts_buffer)
                    disease_buffer = [""] * len(texts_buffer)
                    source_buffer = ["Huatuo26M"] * len(texts_buffer)
                    
                    collection.insert([texts_buffer, vectors_buffer, department_buffer, disease_buffer, source_buffer])
                    
                    # æ›´æ–°è¿›åº¦è®°å½•
                    total_processed_this_run += len(texts_buffer)
                    current_total_line = start_line + total_processed_this_run
                    save_checkpoint(current_total_line)
                    
                    texts_buffer = []
                    vectors_buffer = []

                # æ›´æ–°è¿›åº¦æ¡å±•ç¤º
                pbar.update(len(batch))
                elapsed = time.time() - start_time
                tps = total_processed_this_run / elapsed if elapsed > 0 else 0
                pbar.set_postfix({
                    "TPS": f"{tps:.1f}/s",
                })

    except KeyboardInterrupt:
        print("\næ£€æµ‹åˆ°æ‰‹åŠ¨åœæ­¢ï¼Œæ­£åœ¨ä¿å­˜å½“å‰ç¼“å†²åŒºæ•°æ®...")
        if texts_buffer:
            department_buffer = ["General"] * len(texts_buffer)
            disease_buffer = [""] * len(texts_buffer)
            source_buffer = ["Huatuo26M"] * len(texts_buffer)
            collection.insert([texts_buffer, vectors_buffer, department_buffer, disease_buffer, source_buffer])
            save_checkpoint(start_line + total_processed_this_run + len(texts_buffer))
        print("è¿›åº¦å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨ç»­ä¼ ã€‚")
        return
    except Exception as e:
        print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
        if texts_buffer:
            department_buffer = ["General"] * len(texts_buffer)
            disease_buffer = [""] * len(texts_buffer)
            source_buffer = ["Huatuo26M"] * len(texts_buffer)
            collection.insert([texts_buffer, vectors_buffer, department_buffer, disease_buffer, source_buffer])
            save_checkpoint(start_line + total_processed_this_run + len(texts_buffer))
        return

    # å¤„ç†å‰©ä½™å°¾æ•°
    if texts_buffer:
        department_buffer = ["General"] * len(texts_buffer)
        disease_buffer = [""] * len(texts_buffer)
        source_buffer = ["Huatuo26M"] * len(texts_buffer)
        collection.insert([texts_buffer, vectors_buffer, department_buffer, disease_buffer, source_buffer])
        save_checkpoint(start_line + total_processed_this_run + len(texts_buffer))

    collection.flush()
    print(f"\nâœ… å¤„ç†å®Œæˆï¼å½“å‰é›†åˆå†…æ•°æ®æ€»é‡: {collection.num_entities}")

if __name__ == "__main__":
    # ä¸ºäº†è®© app.core.config èƒ½æ­£å¸¸å·¥ä½œï¼Œéœ€è¦æ·»åŠ è·¯å¾„åˆ° sys.path
    sys.path.append(os.getcwd())
    main()
