=== V11.1 20项全量测试开始 ===
--------------------------------------------------
Testing [中医]: 我最近感觉腰膝酸软，特别怕冷，晚上夜尿也多，想找个老中医调理下身体。
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045348.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045348.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:53:49.496427
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< [INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045352.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045352.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
<think>
1. 患者症状描述：眼睛干涩、发痒，看电脑时间长了就流泪。...
2. 核心体征分析：该症状涉及眼科领域。
3. 结论：建议前往眼科就诊。
</think>
建议挂：眼科。理由：眼睛干涩、发痒，看电脑时间长了就流泪，可能是由于长时间使用电脑导致眼睛疲劳引起的。建议您及时休息，避免长时间盯着电脑，多喝水，多休息，避免揉眼睛，避免长时间使用电脑。如果症状持续或加重，建议及时就医。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:53:53.558727
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< --------------------------------------------------
Testing [JSON-挂号]: 帮我查一下明天上午王医生的挂号情况。
<think>
1. 患者症状描述：我最近感觉腰膝酸软，特别怕冷，晚上夜尿也多，想找个老中医调理下身体。...
2. 核心体征分析：该症状涉及中医领域。
3. 结论：建议前往中医科就诊。
</think>
建议挂：中医科。理由：腰膝酸软、怕冷、夜尿多，可能与肾虚有关，建议找中医师辨证施治，根据体质进行调理。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
--------------------------------------------------
Testing [产科]: 怀孕32周了，今天感觉肚子有点发紧，胎动频繁，正常吗？
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045359.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045359.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:00.904743
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"status": "info", "data": {"doctor_name": "王医生", "appointment_time": "明天上午", "appointment_status": "待确认"}}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045403.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045403.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:04.031505
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< --------------------------------------------------
Testing [JSON-预约]: 帮我预约下周一上午内科的李主任。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
<think>
1. 患者症状描述：怀孕32周了，今天感觉肚子有点发紧，胎动频繁，正常吗？
2. 核心体征分析：该症状涉及妇产科领域。
3. 结论：建议前往产科就诊。
</think>
建议挂：产科。理由：怀孕32周时，胎动频繁可能提示胎儿宫内窘迫，需要及时处理。建议您及时就医，进行产前检查，以确保胎儿健康。同时，注意休息，避免过度劳累，保持良好的心态，有助于胎儿的健康发育。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [儿科]: 三岁小孩发烧38.5度，伴有犬吠样咳嗽，呼吸有点急促。
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045409.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045409.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:10.225688
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"appointment_date": "2023-04-01", "doctor_name": "李主任", "time": "上午"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-报告]: 我想查一下我的核酸检测报告出来了没。
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045413.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045413.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:14.097097
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
<think>
1. 患者症状描述：三岁小孩发烧38.5度，伴有犬吠样咳嗽，呼吸有点急促。...
2. 核心体征分析：该症状涉及儿科领域。
3. 结论：建议前往儿童医院儿科就诊。
</think>
建议挂：儿童医院儿科。理由：三岁小孩发烧38.5度，伴有犬吠样咳嗽，呼吸有点急促。建议及时就医，因为这可能是感冒引起的，需要对症治疗。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
--------------------------------------------------
Testing [急诊]: 我胸口闷得慌，像有大石头压着，左肩膀也跟着疼，出冷汗。
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045418.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045418.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:18.996867
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
<think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"query": "我想查一下我的核酸检测报告出来了没。", "type": "general"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-位置]: 帮我查询一下医院附近的停车场位置。
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045423.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045423.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:23.973255
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< [INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
<think>
1. 患者症状描述：我胸口闷得慌，像有大石头压着，左肩膀也跟着疼，出冷汗。...
2. 核心体征分析：该症状涉及心血管系统。
3. 结论：建议前往心血管内科就诊。
</think>
建议挂：心血管内科。理由：胸口闷得慌，像有大石头压着，左肩膀也跟着疼，出冷汗，可能是心绞痛，建议及时就医。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045426.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045426.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:27.374358
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< --------------------------------------------------
Testing [妇产]: 月经已经推迟了十天，测了试纸是两条杠，我该挂哪个科？
<think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"query": "帮我查询一下医院附近的停车场位置。", "type": "general", "target": "medical", "priority": "normal"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
--------------------------------------------------
Testing [JSON-缴费]: 我想知道怎么在手机上交住院押金。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045432.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045432.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:33.714122
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< [INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045435.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045435.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
<think>
1. 患者症状描述：月经已经推迟了十天，测了试纸是两条杠，我该挂哪个科？...
2. 核心体征分析：该症状涉及妇科。
3. 结论：建议前往妇产科就诊。
</think>
建议挂：妇产科。
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:35.981358
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< --------------------------------------------------
Testing [消化]: 经常胃痛，烧心，吃完饭感觉胀气，反酸水。
<think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"operation_type": "information_request", "query": "如何在手机上交住院押金"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-多语言]: 帮 me 查一下后天上午皮肤科还有没有号。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045441.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045441.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:42.731634
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< [INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045443.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045443.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:44.302110
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
1. 患者症状描述：经常胃痛，烧心，吃完饭感觉胀气，反酸水。...
2. 核心体征分析：该症状涉及消化内科。
3. 结论：建议前往消化内科就诊。
</think>
建议挂：消化内科。理由：胃痛、烧心、胀气、反酸水等症状可能与胃炎、胃溃疡、胃食管反流病等有关。建议您及时就医，进行胃镜检查，以明确病因并进行治疗。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
<think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"query": "帮 me 查一下后天上午皮肤科还有没有号", "type": "general"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [老年]: 家里老人最近总是忘事，出门找不到家，性格也变得孤僻。
--------------------------------------------------
Testing [通用]: 今天天气怎么样，适合出门看病吗？
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045451.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045451.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045452.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045452.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:52.658742
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< [INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:54:52.736544
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
通用知识：今天天气怎么样，适合出门看病吗？
</think>
今天天气晴朗，适合出门看病。建议您在天气晴朗、空气清新、无雾霾的情况下，选择去医院就诊。如果天气阴雨、多雾、多风、有霾、有沙尘等不良天气，建议您避免去医院就诊。
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
<think>
1. 患者症状描述：家里老人最近总是忘事，出门找不到家，性格也变得孤僻。
2. 核心体征分析：该症状涉及心理科学领域。
3. 结论：建议前往精神心理科就诊。
</think>
建议挂：精神心理科。理由：老人忘记事情、出门找不到家、性格孤僻，可能与认知功能下降、情绪障碍或精神疾病有关。建议及时就医，进行相关检查，以明确病因并采取相应的治疗措施。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
=== 测试完成 ===
--------------------------------------------------
Testing [儿科]: 两岁宝宝屁股上长了好多红疹子，又红又肿，是不是湿疹？
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045501.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045501.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:55:02.167850
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
1. 患者症状描述：两岁宝宝屁股上长了好多红疹子，又红又肿，是不是湿疹？
2. 核心体征分析：该症状涉及皮肤科领域。
3. 结论：建议前往皮肤科就诊。
</think>
建议挂：皮肤科。理由：两岁宝宝屁股上长了好多红疹子，又红又肿，可能是湿疹。建议您带宝宝去皮肤科医生处进行检查和诊断，以确定是否为湿疹或其他皮肤问题。医生会根据宝宝的皮肤状况，给出相应的治疗建议。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [内分泌]: 最近总是口渴，喝水多尿也多，体重掉得厉害，我是不是得糖尿病了？
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045510.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045510.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:55:11.447988
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
1. 患者症状描述：最近总是口渴，喝水多尿也多，体重掉得厉害，我是不是得糖尿病了？...
2. 核心体征分析：该症状涉及生理系统，属于内科范畴。
3. 结论：建议前往普通内科就诊。
</think>
建议挂：普通内科。理由：口渴、多尿、体重下降，这些症状可能与糖尿病有关，但不能单独诊断为糖尿病。建议您尽快去医院进行血糖检测，以明确诊断。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [骨科]: 脚踝扭伤了，现在肿得像个大馒头，能不能走路？
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045519.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045519.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:55:20.026495
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
1. 患者症状描述：脚踝扭伤了，现在肿得像个大馒头，能不能走路？...
2. 核心体征分析：该症状涉及外科领域。
3. 结论：建议前往普通外科就诊。
</think>
建议挂：普通外科。理由：扭伤后肿胀明显，脚踝不能正常活动，走路时会感到疼痛，建议在医生指导下进行康复锻炼，避免再次扭伤。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [心理]: 最近总是失眠，心情很烦躁，容易发火，想咨询心理医生。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045527.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045527.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:55:28.580440
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
1. 患者症状描述：最近总是失眠，心情很烦躁，容易发火，想咨询心理医生。...
2. 核心体征分析：该症状涉及心理科学领域。
3. 结论：建议前往精神心理科就诊。
</think>
建议挂：精神心理科。理由：失眠是一种常见的心理问题，可能与压力、焦虑、抑郁等心理因素有关。建议您咨询心理医生，进行心理评估和治疗。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [耳鼻喉]: 孩子最近总说耳朵疼，里面好像有流水，听力也下降了。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045536.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045536.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:55:37.052180
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
1. 患者症状描述：孩子最近总说耳朵疼，里面好像有流水，听力也下降了。...
2. 核心体征分析：该症状涉及神经科学领域。
3. 结论：建议前往神经内科就诊。
</think>
建议挂：神经内科。理由：孩子出现耳朵疼、里面有流水、听力下降等症状，可能是耳部感染或耳部疾病引起的，建议及时就医检查，明确诊断后进行治疗。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [眼科]: 眼睛干涩、发痒，看电脑时间长了就流泪。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045544.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045544.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:55:45.649527
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
1. 患者症状描述：眼睛干涩、发痒，看电脑时间长了就流泪。...
2. 核心体征分析：该症状涉及眼科领域。
3. 结论：建议前往眼科就诊。
</think>
建议挂：眼科。理由：眼睛干涩、发痒，看电脑时间长了就流泪，可能是由于长时间使用电脑导致眼睛疲劳引起的。建议您及时休息，避免长时间盯着电脑，多喝水，多休息，避免揉眼睛，避免长时间使用电脑。如果症状持续或加重，建议及时就医。...
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-挂号]: 帮我查一下明天上午王医生的挂号情况。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045553.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045553.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:55:54.516368
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"status": "info", "data": {"doctor_name": "王医生", "appointment_time": "明天上午", "appointment_status": "待确认"}}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-预约]: 帮我预约下周一上午内科的李主任。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045601.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045601.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:56:02.214675
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"appointment_date": "2023-04-01", "doctor_name": "李主任", "time": "上午"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-报告]: 我想查一下我的核酸检测报告出来了没。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045609.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045609.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:56:09.805469
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"query": "我想查一下我的核酸检测报告出来了没。", "type": "general"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-位置]: 帮我查询一下医院附近的停车场位置。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045616.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045616.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:56:17.240085
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"query": "帮我查询一下医院附近的停车场位置。", "type": "general", "target": "medical", "priority": "normal"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-缴费]: 我想知道怎么在手机上交住院押金。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045624.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045624.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:56:24.875134
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"operation_type": "information_request", "query": "如何在手机上交住院押金"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [JSON-多语言]: 帮 me 查一下后天上午皮肤科还有没有号。
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system 你是一个严格遵循 JSON 格式的医疗接口。`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045631.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='你是一个严格遵循 JSON 格式的医疗接口。', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045631.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: '你是一个严格遵循 JSON 格式的医疗接口。'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:56:32.329708
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
JSON格式化逻辑：提取核心实体并结构化。
</think>
{"query": "帮 me 查一下后天上午皮肤科还有没有号", "type": "general"}
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
--------------------------------------------------
Testing [通用]: 今天天气怎么样，适合出门看病吗？
run sh: `/home/kkk/train_v9/venv_swift/bin/python3 /home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py --model /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875 --stream false --infer_backend pt --system You are a helpful assistant.`
[INFO:swift] Successfully registered `/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] Successfully loaded /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/args.json.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO:swift] Setting args.eval_human: True
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] args.result_path: /home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045639.jsonl
[INFO:swift] Global seed set to 42
[INFO:swift] args: InferArguments(use_ray=False, ray_exp_name=None, device_groups=None, model='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', model_type='qwen3', model_revision=None, task_type='causal_lm', torch_dtype=torch.bfloat16, attn_impl=None, new_special_tokens=[], num_labels=None, problem_type=None, rope_scaling=None, device_map=None, max_memory={}, max_model_len=None, local_repo_path=None, init_strategy=None, template='qwen', system='You are a helpful assistant.', max_length=40960, truncation_strategy='delete', max_pixels=None, agent_template=None, norm_bbox=None, use_chat_template=True, padding_side='right', padding_free=False, loss_scale='default', sequence_parallel_size=1, template_backend='swift', response_prefix=None, enable_thinking=None, add_non_thinking_prefix=True, dataset=[], val_dataset=[], cached_dataset=[], cached_val_dataset=[], split_dataset_ratio=0.0, data_seed=42, dataset_num_proc=1, load_from_cache_file=False, dataset_shuffle=True, val_dataset_shuffle=False, streaming=False, interleave_prob=None, stopping_strategy='first_exhausted', shuffle_buffer_size=1000, download_mode='reuse_dataset_if_exists', columns={}, strict=False, remove_unused_columns=True, model_name=None, model_author=None, custom_dataset_info=[], quant_method=None, quant_bits=None, hqq_axis=None, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, max_new_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stream=False, stop_words=[], logprobs=False, top_logprobs=None, structured_outputs_regex=None, ckpt_dir='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875', lora_modules=[], tuner_backend='peft', train_type='full', adapters=[], external_plugins=[], seed=42, model_kwargs={}, load_args=True, load_data_args=False, packing=False, packing_length=None, packing_num_proc=1, lazy_tokenize=False, custom_register_path=[], use_hf=False, hub_token=None, ddp_timeout=18000000, ddp_backend=None, ignore_args_error=False, use_swift_lora=False, vllm_gpu_memory_utilization=0.9, vllm_tensor_parallel_size=1, vllm_pipeline_parallel_size=1, vllm_enable_expert_parallel=False, vllm_max_num_seqs=256, vllm_max_model_len=None, vllm_disable_custom_all_reduce=True, vllm_enforce_eager=False, vllm_limit_mm_per_prompt=None, vllm_max_lora_rank=16, vllm_enable_prefix_caching=None, vllm_use_async_engine=False, vllm_quantization=None, vllm_reasoning_parser=None, vllm_disable_cascade_attn=False, vllm_mm_processor_cache_gb=None, vllm_speculative_config=None, vllm_engine_kwargs={}, vllm_data_parallel_size=1, sglang_tp_size=1, sglang_pp_size=1, sglang_dp_size=1, sglang_ep_size=1, sglang_enable_ep_moe=False, sglang_mem_fraction_static=None, sglang_context_length=None, sglang_disable_cuda_graph=False, sglang_quantization=None, sglang_kv_cache_dtype='auto', sglang_enable_dp_attention=False, sglang_disable_custom_all_reduce=True, sglang_speculative_algorithm=None, sglang_speculative_num_steps=None, sglang_speculative_eagle_topk=None, sglang_speculative_num_draft_tokens=None, lmdeploy_tp=1, lmdeploy_session_len=None, lmdeploy_cache_max_entry_count=0.8, lmdeploy_quant_policy=0, lmdeploy_vision_batch_size=1, merge_lora=False, safe_serialization=True, max_shard_size='5GB', infer_backend='pt', result_path='/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875/infer_result/20260205-045639.jsonl', write_batch_size=1000, metric=None, max_batch_size=1, val_dataset_sample=None, reranker_use_activation=True)
The tokenizer you are loading from '/home/kkk/train_v11_1/output_v11_1_full/v5-20260205-041040/checkpoint-1875' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'dtype': torch.bfloat16}
[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 40960
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] model: Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 1024, padding_idx=151643)
    (layers): ModuleList(
      (0-27): 28 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
        )
        (mlp): Qwen3MLP(
          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)
          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen3RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)
)
[INFO:swift] Start time of running main: 2026-02-05 04:56:39.769684
[INFO:swift] swift.__version__: 3.12.4
[INFO:swift] request_config: RequestConfig(max_tokens=None, temperature=None, top_k=None, top_p=None, repetition_penalty=None, num_beams=1, stop=[], seed=None, stream=False, logprobs=False, top_logprobs=None, n=1, best_of=None, presence_penalty=0.0, frequency_penalty=0.0, length_penalty=1.0, return_details=False, structured_outputs_regex=None)
[INFO:swift] Input `exit` or `quit` to exit the conversation.
[INFO:swift] Input `multi-line` to switch to multi-line input mode.
[INFO:swift] Input `reset-system` to reset the system and clear the history.
[INFO:swift] Input `clear` to clear the history.
<<< <think>
通用知识：今天天气怎么样，适合出门看病吗？
</think>
今天天气晴朗，适合出门看病。建议您在天气晴朗、空气清新、无雾霾的情况下，选择去医院就诊。如果天气阴雨、多雾、多风、有霾、有沙尘等不良天气，建议您避免去医院就诊。
--------------------------------------------------
<<< Traceback (most recent call last):
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/cli/infer.py", line 5, in <module>
    infer_main()
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 310, in infer_main
    return SwiftInfer(args).main()
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/base.py", line 49, in main
    result = self.run()
             ^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 93, in run
    result = self.infer_cli()
             ^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/infer.py", line 151, in infer_cli
    query = infer_state.input_text()
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kkk/train_v9/venv_swift/lib/python3.12/site-packages/swift/llm/infer/utils.py", line 96, in input_text
    text = input(f'<<<{addi_prompt} ')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
EOFError: EOF when reading a line
=== 测试完成 ===
